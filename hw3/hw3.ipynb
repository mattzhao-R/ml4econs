{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b47cfb64-dddf-40c7-bdbb-a95b3455dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from multiprocessing import cpu_count\n",
    "max_cpu = cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d5903-62eb-4ca1-8701-63014579f736",
   "metadata": {},
   "source": [
    "# Homework (Lecture 9)\n",
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2610eb4-c6b5-4f80-bfa3-cd19fe97d629",
   "metadata": {},
   "source": [
    "### Part (a)\n",
    "\n",
    "$X_i \\sim Exp(\\lambda), \\lambda >0 $\n",
    "\n",
    "$\\displaystyle \\sum_{i=1}^{100} \\log(\\lambda e^{-\\lambda X_i}) \\cdot 1(X_i > 0) = \\sum_{i=1}^{100} \\log(\\lambda e^{-\\lambda X_i}) = 100 log(\\lambda) - \\lambda \\sum_{i=1}^{100} X_i$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\lambda} ( 100 log(\\lambda) - \\lambda \\sum_{i=1}^{100} X_i) = \\frac{100}{\\lambda} - \\sum_i X_i \\Rightarrow \\hat{\\lambda} = \\displaystyle \\frac{1}{\\bar{X}}$\n",
    "\n",
    "To find the asymptotic variance of $\\hat{\\lambda}$, we can apply the Delta Method. In particular, since $\\sqrt{n} (\\bar{X} - \\mu) \\overset{d}{\\to} N(0, \\sigma^2)$, where $\\mu = \\displaystyle \\frac{1}{\\lambda}, \\sigma^2 = \\frac{1}{\\lambda^2}$, we can say that $\\sqrt{n} (\\hat{\\lambda} - \\lambda) \\overset{d}{\\to} N(0, \\sigma^2 \\cdot  (\\frac{1}{-\\frac{1}{\\lambda^2}})^2) = N(0, \\lambda^2)$. The asymptotic variance of the MLE estimator is $\\lambda^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c73ae5-64b4-40d2-a0e8-46de4b3521e5",
   "metadata": {},
   "source": [
    "### Part (b)\n",
    "\n",
    "Given $\\hat{\\lambda}^{MLE}$, we can use the estimated CDF of X to find $\\displaystyle P(X \\le 1) = 1- e^{-\\lambda \\cdot 1} = 1- e^{-\\hat{\\lambda}^{MLE}} = 1- e^{-\\frac{1}{\\bar{X}}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439a5245-4959-40d3-a0e1-866dc9fe4221",
   "metadata": {},
   "source": [
    "### Part (c)\n",
    "\n",
    "Similarly to Part (a), since we have $\\displaystyle \\sqrt{n} (\\bar{X} - \\frac{1}{\\lambda}) \\overset{d}{\\to} N(0, \\frac{1}{\\lambda^2})$, to construct a 95% CI for $P(X \\le 1)$, we can show that $\\displaystyle \\sqrt{n} (\\hat{P} - P) \\overset{d}{\\to} N(0, \\frac{1}{\\lambda^2} \\cdot (1+e^{-\\lambda})^2) = N(0, (\\frac{1+e^{-\\lambda}}{\\lambda})^2 )$\n",
    "\n",
    "Now that we have the asymptotic variance of this estimator, we can construct a 95% CI: $( (1- e^{-\\frac{1}{\\bar{X}}}) - 1.96 \\cdot (\\bar{X}(1+e^{-\\frac{1}{\\bar{X}}})^2), (1- e^{-\\frac{1}{\\bar{X}}}) + 1.96 \\cdot (\\bar{X}(1+e^{-\\frac{1}{\\bar{X}}})^2))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f1eaff-62ff-4dae-9db6-852e38440810",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part (d)\n",
    "\n",
    "$\\hat{P}(X \\le 1) = 1- e^{-\\frac{1}{\\bar{X}}} = 0.8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51060c75-2e5c-4d0b-a204-4138911e5332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "class",
   "language": "python",
   "name": "class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
