{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b47cfb64-dddf-40c7-bdbb-a95b3455dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from multiprocessing import cpu_count\n",
    "max_cpu = cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d5903-62eb-4ca1-8701-63014579f736",
   "metadata": {},
   "source": [
    "# Homework (Lecture 9)\n",
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2610eb4-c6b5-4f80-bfa3-cd19fe97d629",
   "metadata": {},
   "source": [
    "### Part (a)\n",
    "\n",
    "$X_i \\sim Exp(\\lambda), \\lambda >0 $\n",
    "\n",
    "$\\displaystyle \\sum_{i=1}^{100} \\log(\\lambda e^{-\\lambda X_i}) \\cdot 1(X_i > 0) = \\sum_{i=1}^{100} \\log(\\lambda e^{-\\lambda X_i}) = 100 log(\\lambda) - \\lambda \\sum_{i=1}^{100} X_i$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\lambda} ( 100 log(\\lambda) - \\lambda \\sum_{i=1}^{100} X_i) = \\frac{100}{\\lambda} - \\sum_i X_i \\Rightarrow \\hat{\\lambda} = \\displaystyle \\frac{1}{\\bar{X}}$\n",
    "\n",
    "To find the asymptotic variance of $\\hat{\\lambda}$, we can apply the Delta Method. In particular, since $\\sqrt{n} (\\bar{X} - \\mu) \\overset{d}{\\to} N(0, \\sigma^2)$, where $\\mu = \\displaystyle \\frac{1}{\\lambda}, \\sigma^2 = \\frac{1}{\\lambda^2}$, we can say that $\\sqrt{n} (\\hat{\\lambda} - \\lambda) \\overset{d}{\\to} N(0, \\sigma^2 \\cdot  (\\frac{1}{-\\frac{1}{\\lambda^2}})^2) = N(0, \\lambda^2)$. The asymptotic variance of the MLE estimator is $\\lambda^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c73ae5-64b4-40d2-a0e8-46de4b3521e5",
   "metadata": {},
   "source": [
    "### Part (b)\n",
    "\n",
    "Given $\\hat{\\lambda}^{MLE}$, we can use the estimated CDF of X to find $\\displaystyle P(X \\le 1) = 1- e^{-\\lambda \\cdot 1} = 1- e^{-\\hat{\\lambda}^{MLE}} = 1- e^{-\\frac{1}{\\bar{X}}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439a5245-4959-40d3-a0e1-866dc9fe4221",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part (c)\n",
    "\n",
    "Similarly to Part (a), since we have $\\displaystyle \\sqrt{n} (\\bar{X} - \\frac{1}{\\lambda}) \\overset{d}{\\to} N(0, \\frac{1}{\\lambda^2})$, to construct a 95% CI for $P(X \\le 1)$, we can show that $\\displaystyle \\sqrt{n} (\\hat{P} - P) \\overset{d}{\\to} N(0, \\frac{1}{\\lambda^2} \\cdot (1+e^{-\\lambda})^2) = N(0, (\\frac{1+e^{-\\lambda}}{\\lambda})^2 )$\n",
    "\n",
    "Now that we have the asymptotic variance of this estimator, we can construct a 95% CI: $\\hat{p} \\pm 1.96 \\times se(\\hat{p}) = 1- e^{-\\frac{1}{\\bar{X}}} \\pm 1.96 \\times \\frac{\\sigma}{\\sqrt{n}} = 0.9179 \\pm 0.08484$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f1eaff-62ff-4dae-9db6-852e38440810",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part (d)\n",
    "\n",
    "$\\hat{P}(X \\le 1) = 0.8 \\Rightarrow \\frac{1}{n} \\sum_{i=1}^{100} 1(X \\le 1) = 0.8$. Since the sum of Bernoulli random variables follows a binomial distribution, we know the variance is $npq = (100)(0.8)(0.2) = 16$. By the CLT, we see that $\\hat{p}_n \\sim N(\\mu, \\frac{\\sigma^2}{n}) = N(0.8,0.16)$. As such, a 95% CI for $\\hat{p}_n$ is: $\\hat{p}_n \\pm 1.96 \\times se(\\hat{p}_n) = 0.8 \\pm 0.0784$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f20943-f5db-4bd1-8e81-82bf4d8c97e4",
   "metadata": {},
   "source": [
    "### Part (e)\n",
    "\n",
    "I would trust the one from Part (d) more since it makes fewer assumptions about the true distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b04fc31-cad7-4bfe-b83f-d6ee93a08653",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "### Part (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faf11c8f-6cf1-4ea4-92f7-16ad1c92656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DGP():\n",
    "    V = np.random.exponential(scale=2,size = 200)\n",
    "    X = 1 + V\n",
    "    epsilon = np.random.normal(0,np.sqrt(0.5),size=200)\n",
    "    Y = 2 + 2 * np.log(X) + epsilon\n",
    "    return X, Y\n",
    "\n",
    "def gauss_kernel(u):\n",
    "    return (1/np.sqrt(2*np.pi)) * np.exp(-(u ** 2)/2)\n",
    "\n",
    "def LL(X,Y,x,h):\n",
    "    Z = np.column_stack((np.ones((1,200)),X-x))\n",
    "    K = np.diag(gauss_kernel((X-x)/h))\n",
    "    m_vec = np.linalg.inv(Z.T @ K @ Z) @ Z.T @ K @ Y\n",
    "    return m_vec[0], m_vec[1]\n",
    "\n",
    "def NN(X,Y,x,k):\n",
    "    x_norm = np.abs(X-x)\n",
    "    m = (1/k) * np.sum(np.sort(np.column_stack((x_norm,Y)),axis=0)[-k:,1])\n",
    "    return m\n",
    "\n",
    "def series(X,Y,x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa13834-7d9c-45d7-b2b2-4318a10ab377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.BSpline.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd40e6-02e6-4e99-8fd7-e3a1ac4efa0f",
   "metadata": {},
   "source": [
    "# Homework (Lecture 11)\n",
    "\n",
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c102e5-6dae-4caf-92d7-802c2f6ad40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31069796-eef1-413c-8309-2c75e02ff1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DGP():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da222063-383e-45af-a0f5-5e82b5e144fe",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "**DON'T FORGET TO SET RANDOM STATE FOR EACH MODEL**\n",
    "\n",
    "- for tree, use ccp_alpha parameter for tuning - Minimal cost-complexity pruning based on Chapter 3 of Breiman et al (1984)\n",
    "- for rf, select num regressors for each split p using max_features: The number of features to consider when looking for the best split\n",
    "- for boosted tree, choose number of leaves using max_leaf_nodes parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6281c4f6-d9cc-4530-bdff-33632cf110bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=100, random_state=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c292d44e-a853-4e0f-8f7e-c739e675e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f78ba-ce85-413a-8b8a-515fa173050e",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "### Part (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68f03da3-d61a-4c68-a6dd-b8b2b83d9de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('penn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a80616e-bb67-4292-b7a2-468d33e07b63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "class",
   "language": "python",
   "name": "class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
